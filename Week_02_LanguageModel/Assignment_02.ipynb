{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-02, Probability Model A First Look: An Introduction of Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "1. Review the course online programming code;\n",
    "2. Review the main questions;\n",
    "3. Using wikipedia corpus to build a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this part, you should re-code the programming task in our online course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** Please refer to the file ['LectureCode_02.ipynb'](https://github.com/pchen12567/AI_For_NLP/blob/master/Week_02_LanguageModel/LectureCode_02.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. How to Github and Why do we use Jupyter and Pycharm?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br> \n",
    "Github is a convienient online version control tool, in which we could manage the program version easily, such as check any modification in ecah version or roll back to the specific version. Besides, it is a good way to share work with team with following methods: make branch, merge, pull requests and so on.<br>\n",
    "Jupyter is an online coding notebook, which is convinent for displaying the result and making notes with Markdown language.<br>\n",
    "Pycharm is a Python IDE tool which means Integrated Development Environment for Python, it is convinent for developing large program and has good features about debuging, gitting, deployting different configurations and other complex but practical functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. What's the Probability Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Can you came up with some sceneraies at which we could use Probability Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "1. Spam Email Classification\n",
    "2. Loan apparoval system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "- Easy to implement;\n",
    "- Performs well for small scale data;\n",
    "- Insensitive with missing values.\n",
    "\n",
    "\n",
    "Can't match all the situation with limited patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5. What's the Language Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "Models that assign probabilities to sequences of words are called language models or LMs.<br>\n",
    "简单说，语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否是人话的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6. Can you came up with some sceneraies at which we could use Language Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "1. Auto reply email\n",
    "2. Auto words completion\n",
    "3. Text error detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***7. What's the 1-gram language model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "An N-gram is a sequence of N words: a 1-gram (or Unigram) is an one-word sequence of words like spliting the sentence `\"I am writting my assignment\"` to each signal word: `['I', 'am', 'writting', 'my', 'assignment']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***8. What's the disadvantages and advantages of 1-gram language mode?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "Advantages:\n",
    "- Easy to compute\n",
    "- Fast training rate\n",
    "\n",
    "Disadvantages:\n",
    "- Low accuracy\n",
    "- Should assume all words are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***9. What't the 2-gram models?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "An N-gram is a sequence of N words: a 2-gram (or biggram) is a two-word sequence of words like spliting the sentence `\"I am writting my assignment\"` to each pair words: `['I am', 'am writting', 'writting my', 'my assignment']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***10. What's the web crawler, and can you implement a simple crawler?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "The web crawler is a program script used to simulate the human actions to scan the target web site content and match the pattern to get the specific information automatically by machine.\n",
    "\n",
    "Simple Crawler:<br>\n",
    "1. Set target url: `url = 'url'`\n",
    "2. Set pattern with regular expression: `pattern = re.function('regular expression')`\n",
    "3. Get response from target web: `response = requests.get(url)`\n",
    "4. Match the pattern to get info: `info = pattern.findall(response.text)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***11. There may be some issues to make our crawler programming difficult, what are these, and how do we solve them?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "1. Might be blocked if crawl too frequently: set time sleep\n",
    "2. Heavy content such as videos and images.\n",
    "3. Javascript and other forms of dynamic webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***12. What't the Regular Expression and how to use?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "A regular expression is an algebraic notation for characterizing a set of strings. They are particularly useful for searching in texts, when we have a pattern to search for and a corpus of corpus texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: You need to download the corpus from wikipedis:\n",
    "> https://dumps.wikimedia.org/zhwiki/20190401/\n",
    "\n",
    "Step 2: You may need the help of wiki-extractor:\n",
    "> https://github.com/attardi/wikiextractor\n",
    "\n",
    "Step 3: Using the technologies and methods to finish the language model;\n",
    "\n",
    "Step 4: Try some interested sentence pairs, and check if your model could fit them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone WikiExtractor to local<br>\n",
    "`$ git clone https://github.com/attardi/wikiextractor.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install WikiExtractor<br>\n",
    "`$ cd wikiextractor/`<br>\n",
    "`$ python3 setup.py install`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data<br>\n",
    "`$ python3 WikiExtractor.py -b 2000M -o ~/AI_For_NLP/Assignment_02/data/ --no-templates --processes 8 ~/Downloads/zhwiki-20190401-pages-articles-multistream.xml.bz2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "usage: WikiExtractor.py [-h] [-o OUTPUT] [-b n[KMG]] [-c] [--json] [--html]\n",
    "                        [-l] [-s] [--lists] [-ns ns1,ns2]\n",
    "                        [--templates TEMPLATES] [--no-templates] [-r]\n",
    "                        [--min_text_length MIN_TEXT_LENGTH]\n",
    "                        [--filter_disambig_pages] [-it abbr,b,big]\n",
    "                        [-de gallery,timeline,noinclude] [--keep_tables]\n",
    "                        [--processes PROCESSES] [-q] [--debug] [-a] [-v]\n",
    "                        input\n",
    "\n",
    "-b, --bytes= n[KM]      Put specified bytes per output file (default 1M)\n",
    "-o, --output= dir       Place output files in specified directory (default current)\n",
    "--no-templates          Do not expand templates\n",
    "--processes PROCESSES   Number of processes to use (default 1)\n",
    "input                   XML wiki dump file\n",
    "\n",
    "\n",
    "Full Introduction:\n",
    "Wikipedia Extractor:\n",
    "Extracts and cleans text from a Wikipedia database dump and stores output in a\n",
    "number of files of similar size in a given directory.\n",
    "Each file will contain several documents in the format:\n",
    "\n",
    "    <doc id=\"\" revid=\"\" url=\"\" title=\"\">\n",
    "        ...\n",
    "        </doc>\n",
    "\n",
    "If the program is invoked with the --json flag, then each file will\n",
    "contain several documents formatted as json ojects, one per line, with\n",
    "the following structure\n",
    "\n",
    "    {\"id\": \"\", \"revid\": \"\", \"url\":\"\", \"title\": \"\", \"text\": \"...\"}\n",
    "\n",
    "Template expansion requires preprocesssng first the whole dump and\n",
    "collecting template definitions.\n",
    "\n",
    "positional arguments:\n",
    "  input                 XML wiki dump file\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --processes PROCESSES\n",
    "                        Number of processes to use (default 1)\n",
    "\n",
    "Output:\n",
    "  -o OUTPUT, --output OUTPUT\n",
    "                        directory for extracted files (or '-' for dumping to\n",
    "                        stdout)\n",
    "  -b n[KMG], --bytes n[KMG]\n",
    "                        maximum bytes per output file (default 1M)\n",
    "  -c, --compress        compress output files using bzip\n",
    "  --json                write output in json format instead of the default one\n",
    "\n",
    "Processing:\n",
    "  --html                produce HTML output, subsumes --links\n",
    "  -l, --links           preserve links\n",
    "  -s, --sections        preserve sections\n",
    "  --lists               preserve lists\n",
    "  -ns ns1,ns2, --namespaces ns1,ns2\n",
    "                        accepted namespaces in links\n",
    "  --templates TEMPLATES\n",
    "                        use or create file containing templates\n",
    "  --no-templates        Do not expand templates\n",
    "  -r, --revision        Include the document revision id (default=False)\n",
    "  --min_text_length MIN_TEXT_LENGTH\n",
    "                        Minimum expanded text length required to write\n",
    "                        document (default=0)\n",
    "  --filter_disambig_pages\n",
    "                        Remove pages from output that contain disabmiguation\n",
    "                        markup (default=False)\n",
    "  -it abbr,b,big, --ignored_tags abbr,b,big\n",
    "                        comma separated list of tags that will be dropped,\n",
    "                        keeping their content\n",
    "  -de gallery,timeline,noinclude, --discard_elements gallery,timeline,noinclude\n",
    "                        comma separated list of elements that will be removed\n",
    "                        from the article text\n",
    "  --keep_tables         Preserve tables in the output article text\n",
    "                        (default=False)\n",
    "\n",
    "Special:\n",
    "  -q, --quiet           suppress reporting progress info\n",
    "  --debug               print debug info\n",
    "  -a, --article         analyze a file containing a single article (debug\n",
    "                        option)\n",
    "  -v, --version         print program version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch Traditional Chinese to Simple Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install OpenCC<br>\n",
    "`$ brew install OpenCC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch<br>\n",
    "`$ opencc -i wiki_corpus -o wiki_corpus_simple.txt -c t2s.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Usage: \n",
    "\n",
    "   OpenCC  [--noflush <bool>] [-i <file>] [-o <file>] [-c <file>] [--]\n",
    "           [--version] [-h]\n",
    "\n",
    "Options: \n",
    "\n",
    "   --noflush <bool>\n",
    "     Disable flush for every line\n",
    "\n",
    "   -i <file>,  --input <file>\n",
    "     Read original text from <file>.\n",
    "\n",
    "   -o <file>,  --output <file>\n",
    "     Write converted text to <file>.\n",
    "\n",
    "   -c <file>,  --config <file>\n",
    "     Configuration file\n",
    "\n",
    "   --,  --ignore_rest\n",
    "     Ignores the rest of the labeled arguments following this flag.\n",
    "\n",
    "   --version\n",
    "     Displays version information and exits.\n",
    "\n",
    "   -h,  --help\n",
    "     Displays usage information and exits.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/wiki_corpus_simple.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Bulid function with regular expression to get words and numbers\n",
    "def token(string):\n",
    "    return ''.join(re.findall('[\\w|\\d]+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/jc/l9vx9tp979g0tm976wjrgwkr0000gn/T/jieba.cache\n",
      "Loading model cost 0.951 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "content = ''\n",
    "all_tokens = []\n",
    "with open(path) as file:\n",
    "    for line in file:\n",
    "        # To skip the article title line\n",
    "        if line.strip().startswith('</doc>') or line.strip().startswith('<doc'): continue\n",
    "        if line:\n",
    "            # To keep just words and numbers\n",
    "            line = token(line)\n",
    "            content += line.strip()\n",
    "            all_tokens += list(jieba.cut(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393440272"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'数学数学是利用符号语言研究数量结构变化以及空间等概念的一门学科从某种角度看属于形式科学的一种数学透过抽象化和逻辑推理的使用由计数计算量度和对物体形状及运动的观察而产生数学家们拓展这些概念为了公式化新的猜想以及从选定的公理及定义中建立起严谨推导出的定理基础数学的知识与运用总是个人与团体生活中不可或缺的一环对数学基本概念的完善早在古埃及美索不达米亚及古印度内的古代数学文本便可观见而在古希腊那里有更为严谨的处理从那时开始数学的发展便持续不断地小幅进展至16世纪的文艺复兴时期因为新的科学发现和数学革新两者的交互致使数学的加速发展直至今日数学并成为许多国家及地区的教育范畴中的一部分今日数学使用在不同的领域中包括科学工程医学经济学和金融学等数学对这些领域的应用通常被称为应用数学有时亦会激起新的数学发现并导致全新学科的发展例如物理学的实质性发展中建立的某些理论激发数学家对于某些问题的不同角度的思考数学家也研究纯数学就是数学本身的实质性内容而不以任何实际应用为目标虽然许多研究以纯数学开始但其过程中也发现许多应用之处西方语言中数学一词源自于古希腊语的其有学习学问科学以及另外还有个较狭义且技术性的意思数学'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197976896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['数学', '数学', '是', '利用', '符号语言', '研究', '数量', '结构', '变化', '以及']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_Gram Model (Unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(W_oW_1W_2Wn) = P(W_o) \\cdot P(W_1) \\cdot P(W_2) \\cdot P(W_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Words Frequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the words frequences\n",
    "words_count = Counter(all_tokens) # return a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time for repeating, write the result to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write file\n",
    "# with open('./data/assignment_1_gram.json', 'w', encoding = 'utf-8') as f:\n",
    "#     json.dump(words_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load file\n",
    "# with open('./data/assignment_1_gram.json', 'r') as f:\n",
    "#     words_count = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_count = Counter(words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get One Word Probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197976896"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute total words frequences\n",
    "frequences_sum = sum(words_count.values())\n",
    "frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function to get word probability\n",
    "def get_prob(word): \n",
    "    \n",
    "    if word in words_count: \n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        # return default prob if word not in words_count \n",
    "        return 1 / frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00024670050388101854"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "get_prob('我们')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Probability Product of Each Single Word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build 1_Gram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_one_gram(sentence):\n",
    "    words = list(jieba.cut(sentence))\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 3.894746636557851e-55\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 2.4900527240252674e-52\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 5.177293045924465e-26\n",
      "---- 真是一只好看的小猫 with probility 4.489874224289105e-23\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 我去吃火锅，今晚 with probility 1.0791980643474994e-28\n",
      "---- 今晚我去吃火锅 with probility 2.1365628294872618e-20\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 2.0642633453607667e-21\n",
      "---- 养乐多绿来一杯 with probility 1.1665050390665548e-20\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"我去吃火锅，今晚 今晚我去吃火锅\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_one_gram(s1), language_model_one_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_Gram Model (Bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(w_ow_1w_2...w_n) = Pr(w_1 | w_0) \\cdot Pr(w_2 | w_1) ... \\cdot Pr(w_n | w_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(w_1 | w_0) =  \\frac { Pr(w_0 \\cdot w_1) }{Pr(w_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get 2_Gram Words Segment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2_gram words list\n",
    "all_2_gram_words = [''.join(all_tokens[i:i+2]) for i in range(len(all_tokens[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197976894"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['数学数学', '数学是', '是利用', '利用符号语言', '符号语言研究']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_2_gram_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2_gram words frequences\n",
    "two_gram_count = Counter(all_2_gram_words) # return a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because of the limitation of RAM, write a python scripyt to get 2_gram words frequencies directly.<br>\n",
    "Please refer to the file 'get_two_gram.py'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time for repeating, write the result to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write file\n",
    "# with open('./data/assignment_2_gram.json', 'w', encoding = 'utf-8') as f:\n",
    "#     json.dump(two_gram_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Load file\n",
    "# with open('./data/assignment_2_gram.json', 'r') as f:\n",
    "#     two_gram_count = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# two_gram_count = Counter(two_gram_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Combination Probability of 2_Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48022526"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(two_gram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187487525"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get total 2_gram frequences\n",
    "total_2_gram_frequences = sum(two_gram_count.values())\n",
    "total_2_gram_frequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combination_prob(previous, word):\n",
    "    if previous + word in two_gram_count:\n",
    "        return two_gram_count[previous + word] / total_2_gram_frequences\n",
    "    else:\n",
    "        # return default prob if two_gram not in two_gram_count\n",
    "        return 1 / total_2_gram_frequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Probability of 2_Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_2_gram(previous, word):\n",
    "    return get_combination_prob(previous, word) / get_prob(previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build 2_Gram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_two_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words = list(jieba.cut(sentence))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上吃大餐 is more possible\n",
      "---- 今天晚上吃大餐 with probility 4.7385558028003305e-14\n",
      "---- 今天晚上吃大葱 with probility 5.923194753500413e-15\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 3.925729444103904e-22\n",
      "---- 真是一只好看的小猫 with probility 1.4734616280399357e-18\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 5.2694590728042534e-15\n",
      "---- 今晚火锅去吃我 with probility 3.39209998421205e-17\n",
      "这个作业很费时间 is more possible\n",
      "---- 这个作业很费时间 with probility 3.423923736432339e-16\n",
      "---- 时间很这个作业费 with probility 2.431303686993877e-19\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上吃大餐 今天晚上吃大葱\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"这个作业很费时间 时间很这个作业费\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_two_gram(s1), language_model_two_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: If we need to solve following problems, how can language model help us?\n",
    "* Voice Recognization.\n",
    "* Sogou pinyin input.\n",
    "* Auto correction in search engine.\n",
    "* Anormal Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "- Voice Recognization: When detecting voice order, to sorted the probability of the sentences and retrun the highest one, which could ensure the accuracy of voice recognization.\n",
    "- Sogou pinyi input: Before the customer selects result after typing pinying, the software could display the result by the order of probability. Besides, When the customer is typing, the software could complete the others automatically according to the existing words.\n",
    "- Auto correction in search engine: The same as Sogou.\n",
    "- Anormal Detection: Set a P_value, compute each sentence's probability of an article, return warning if the probability is lower than P_value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**<br>\n",
    "Advantages:\n",
    "- Much more accurate.\n",
    "- Without setting rules manually.\n",
    "\n",
    "Disadvantages:\n",
    "- Need large language corpus.\n",
    "- Cost much more time to train data.\n",
    "- Out of vocabulary problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) How to solve OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this out-of-vocabulary(OOV) problems. There are so many intelligent man to solve this probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "+ https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation,\n",
    "+ https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf , Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q1: How did you solve this problem in your programming task?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** Set the probability value to 1/N, which N means the total frequences of each N-gram in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
